<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>UCL增强学习课程笔记[2]</title>
  <meta name="description" content="1: Introduction to Reinforcement Learning课程网站课件+视频">
  <meta name="author" content="Wei Wang">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="UCL增强学习课程笔记[2]">
  <meta name="twitter:description" content="1: Introduction to Reinforcement Learning课程网站课件+视频">
  
  <meta property="og:type" content="article">
  <meta property="og:title" content="UCL增强学习课程笔记[2]">
  <meta property="og:description" content="1: Introduction to Reinforcement Learning课程网站课件+视频">
  
  <link rel="icon" type="image/png" href="/assets/images/favicon.png" />
  <link href="/assets/images/favicon.png" rel="shortcut icon" type="image/png">
  
  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/2017/11/RL1-notes/">
  <link rel="alternate" type="application/rss+xml" title="I'm WRong" href="http://localhost:4000/feed.xml">
  
  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />
  
</head>


  <body>

    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>
    
    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/assets/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/#blog" title="前往 I'm WRong 的主页" class="blog-button"><img src="/assets/images/avatar.jpg" width="80" alt="I'm WRong logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for I'm WRong" class="blog-button">I'm WRong</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">The meaning of life is that it stops.</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">AI, Programming, Reading, Gym and Beautiful things.</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        <p class="panel-cover__description"><a href="https://github.com/rongfeng-china/" target="_blank">Nothing</a></p>
        
        
        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="Visit blog" class="blog-button">Blog</a></li>
                
              </ul>
            </nav>
          </div>
          
          <div><nav class="cover-navigation navigation--social">
  <ul class="navigation">

  

  
  <!-- Github -->
  <li class="navigation__item">
    <a href="https://github.com/rongfeng-china" title="@rongfeng-china 的 Github" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>
  
  
  

  

  <!-- RSS -->
  <li class="navigation__item">
    <a href="/feed.xml" rel="author" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>

  
  <!-- Email -->
  <li class="navigation__item">
    <a href="mailto:misswrongwong(at)gmail.com" title="Contact me">
      <i class='social fa fa-envelope'></i>
      <span class="label">Email</span>
    </a>
  </li>
  

  </ul>
</nav>
</div>
        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-red"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-11-29 12:45:00 -0700" itemprop="datePublished" class="post-meta__date date">2017-11-29</time> &#8226; <span class="post-meta__tags tags">RL</span>
    </div>
    <h1 class="post-title">UCL增强学习课程笔记[2]</h1>
  </header>

  <section class="post">
    <h2 id="1-introduction-to-reinforcement-learning">1: Introduction to Reinforcement Learning</h2>
<p>课程网站<a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html">课件+视频</a></p>

<h2 id="1-机器学习的分支">1. 机器学习的分支：</h2>
<p>1.监督式学习（有标签），2.非监督式学习（无标签），3.增强学习（无标签，有环境反馈) <br />
增强学习和其他方法不同之处在于：</p>

<ul>
  <li>不像监督式学习有标签，它只有奖励信号(reward signal)</li>
  <li>反馈是延迟的，而不是立刻的</li>
  <li>时间很重要</li>
  <li>agent（可以理解为机器人，或者在图2中的人脑）的行为影响接下来一系列接受到的反馈数据
<img src="/pics/L1-1.png" alt=" " title="machine learning branches" height="260px" width="350px" /></li>
</ul>

<h2 id="2-增强学习基本概念">2. 增强学习基本概念</h2>
<p>典型的增强学习场景：Agent（人脑）控制动作，这些动作作用于环境（地球），环境给予反馈。人脑通过观察环境，和分析奖惩，再做下一个决定。<em>这在日常生活中很常见，也很容易理解。比如你这一秒在写作业，下一秒决定去玩（动作），结果会被爸妈打（反馈），所以被迫回来继续写作业（下一个动作）。当然了，你不会一下子就那么听话的，经过多次反馈，你就学会了对于自己有益的策略。</em></p>

<ul>
  <li>在时刻t的时候, agent人脑：
    <ul>
      <li>执行动作 $ A_t $</li>
      <li>接收到环境的观测值$O_t$</li>
      <li>接收到环境的奖励$R_t$</li>
    </ul>
  </li>
  <li>环境：
    <ul>
      <li>接收到行为$A_t$</li>
      <li>释放观测信号$O_{t+1}$</li>
      <li>释放奖励信号$R_{t+1}$</li>
    </ul>
  </li>
  <li>t 时间增加
<img src="/pics/L1-2.png" alt=" " title="RL basic concetps" /></li>
</ul>

<h2 id="3-对于变量的解释">3. 对于变量的解释</h2>
<p>随着时间的增长，你会有很多关于观测$O_t$，行为$A_t$和奖励$R_t$的序列，我们可以称它为历史$H_t$。
<script type="math/tex">H_t = O_1, R_1, A_1, ... , A_{t-1}, O_{t-1},R_{t-1}</script>
在增强学习里，你常会遇见一个$S_t$状态变量，这个状态变量就是对于历史的综合处理$S_t = f(H_t)$， 它可以用来对未来发生的事情做判断和决定。我们不是常说从历史中吸取经验教训嘛，就是这个道理。</p>

<p><em>其实我很佩服Rich Sutton，他提出的增强学习模型很简单，但是很贴近人脑的运作方式。比如说从历史中学习经验教训，还有explore和exploit的行为模式（人在有些时候会探索未知、尝试新鲜的事情，另外一些时候我们会采取对于自己最优的策略。）从某种意义上，我觉得他的算法是很哲学的思考的结果。</em></p>

<p>$S_t$具体分有环境的状态$S_t^e$和Agent的状态$S_t^a$，当环境是完全可观测的（fully observable)，也就是Agent能直接看到环境的状态的时候，$S_t^e$和$S_t^a$就是等同的。然而，如果环境是部分可观测的(partially observable)，Agent获得到的就不是直接的环境状态。</p>

<h2 id="4-增强学习agent的组成">4. 增强学习Agent的组成</h2>
<p>一个增强学习的Agent可能包括一个或者更多个以下的组成部分：</p>
<ul>
  <li><strong>策略（Policy)</strong>: Agent如何做行为决策</li>
  <li><strong>价值函数(Value Function)</strong>: 判断一个状态 与/或 一个行为的好坏</li>
  <li><strong>模型(Model)</strong>: Agent对于环境的表达</li>
</ul>

<p><strong>策略（Policy)</strong></p>
<ul>
  <li>它是Agent的行为方式。</li>
  <li>给定一个状态S，经过策略，可以得到一个行为A。所以策略是从状态到行为的映射。</li>
  <li>决定策略 (Determitistic Policy),在状态S下，Agent一定会执行动作a。<br />
$a = \pi(s)$</li>
  <li>随机策略 (Stochastic Policy)， 在状态s下，Agent执行a的概率。<br />
$\pi(a|s)=p[A_t = a| S_t = s] $</li>
</ul>

<p><strong>价值函数(Value Function)</strong></p>
<ul>
  <li>它是对于未来奖惩的预测</li>
  <li>被用于评估一个状态的好坏</li>
  <li>因此被用于选择行为A，例如以下是对于未来获得奖励的一个期望：<br />
$V_{\pi}(s) = E_{\pi}[ R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+…. | S_t = s]$</li>
</ul>

<p><strong>模型(Model)</strong>:</p>
<ul>
  <li>它是用于预测接下来环境会做什么</li>
  <li>P 预测下一个状态</li>
  <li>R 预测下一个即刻奖惩 (reward),例如：
$ P_{ss’}^a = P[S_{t+1}= s’ | S_t = s , A_t = a ]$<br />
$ R_s^a = E [R_{t+1}| S_t = s, A_t = a ]$</li>
</ul>

<p>接下来课件里举了一个迷宫的例子(Maze example)，帮助你理解之前介绍的所有概念，可以跟着课件细细看，这里就不细讲了。</p>

<h2 id="5-增强学习的分类">5. 增强学习的分类</h2>
<ul>
  <li>基于价值(value based)
    <ul>
      <li>价值函数</li>
    </ul>
  </li>
  <li>基于策略(policy based)
    <ul>
      <li>策略方法</li>
    </ul>
  </li>
  <li>演员评判家 (Actor Critic )
    <ul>
      <li>既有价值函数，又有策略方法</li>
    </ul>
  </li>
</ul>

<p>另一种是根据模型来分类：Model Free是指不需要去猜测environment的工作方式，而Model based则是需要学习environment的工作方式。</p>

<p><img src="/pics/L1-3.png" alt=" " title="RL categorizing" /></p>

<h2 id="6-增强学习和规划planning的异同">6. 增强学习和规划(planning)的异同</h2>
<ul>
  <li>增强学习
    <ul>
      <li>环境是部分已知</li>
      <li>Agent和环境交互</li>
      <li>Agent改进自己的策略</li>
    </ul>
  </li>
  <li>规划
    <ul>
      <li>环境模型已知</li>
      <li>Agent根据模型进行计算（不交互）</li>
      <li>Agent 改进自己的策略</li>
    </ul>
  </li>
</ul>

<h2 id="7-探索和利用exploration-and-exploitation">7. 探索和利用(Exploration and Exploitation)</h2>
<p>总而言之，增强学习是一种试错学习（trial-and-error), Agent能够通过和环境交互中的经验，发现一个好的策略。增强学习选择动作，包括探索和利用。探索是为了发现更多关于环境的信息，利用是为了充分利用已有的经验而最大化奖励。</p>

<p><a href="http://localhost:4000/2017/11/RL_notes/">更多RL文章</a></p>

  </section>
</article>

<section class="read-more">
   
   
   
   
   <div class="read-more-item">
       <span class="read-more-item-dim">更早的文章</span>
       <h2 class="post-list__post-title post-title"><a href="/2017/11/RL_notes/" title="link to UCL增强学习课程笔记, David Silver">UCL增强学习课程笔记, David Silver</a></h2>
       <p class="excerpt">UCL 增强学习课程 – David Silver课程网站课件+视频	Lecture 1: Introduction to Reinforcement Learning	Lecture 2: Markov Decision Processes	Lecture 3: Planning by Dynamic Programming	Lecture 4: Model-Free Prediction	Lecture 5: Model-Free Control	Lecture 6: Value Fu...&hellip;</p>
       <div class="post-list__meta"><time datetime="2017-11-27 12:45:00 -0700" class="post-list__meta--date date">2017-11-27</time> &#8226; <span class="post-list__meta--tags tags">RL</span><a class="btn-border-small" href=/2017/11/RL_notes/>继续阅读</a></div>
   </div>
   
</section>

<section class="post-comments">
  
    <div id="disqus_thread"></div>
    <script>
    
    var disqus_config = function () {
        this.page.url = "http://localhost:4000/2017/11/RL1-notes/";
        this.page.identifier = "/2017/11/RL1-notes/";
    };

    var disqus_shortname = 'vno-jekyll';
    
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>要查看<a href="http://disqus.com/?ref_noscript"> Disqus </a>评论，请启用 JavaScript</noscript>
    
  
  
  
  
</section>


            <section class="footer">
    <footer>
    	<span class="footer__copyright">本站点采用<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享 署名-非商业性使用-相同方式共享 4.0 国际 许可协议</a></span>
        <span class="footer__copyright">由 <a href="https://jekyllrb.com">Jekyll</a> 于 2017-12-04 生成，感谢 <a href="https://www.digitalocean.com/?refcode=30ed2d146762">Digital Ocean</a> 为本站提供稳定的 VPS 服务</span>
        <span class="footer__copyright">本站由 <a href="http://rongfeng-china.github.io">@rong</a> 创建，采用 <a href="https://github.com/onevcat/vno-jekyll">Vno - Jekyll</a> 作为主题，您可以在 GitHub 找到<a href="https://github.com/onevcat/OneV-s-Den">本站源码</a> - &copy; 2017</span>
    </footer>
</section>

        </div>
    </div>
    
    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>

<script type="text/javascript" src="/js/main.js"></script>



    
  </body>

</html>

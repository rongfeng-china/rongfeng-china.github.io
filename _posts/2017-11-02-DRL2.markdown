---
layout: post
title: Introduction and course overview [1]
date: 2017-11-01 20:45:00 +01:00
categories: tech
tags: DRL
---
## 第一节: Introduction and course overview
*Berkeley课程网站[课件+视频](http://rll.berkeley.edu/deeprlcourse/)\\
第一节是对基本概念的介绍和名词的解释，包括deep learning, reinforcement learning 和 deep reinforcement learning。*

**1. 什么是Deep RL？** \\
答: Deep RL包含了deep learing（深度学习）和reinforcement learning（增强学习）。

**2. 什么是Reinforcement learning？** \\
答：增强学习就通过和环境互动，从环境中获得奖励来学习的一种方式。很通俗的一个例子是我们训练狗的时候，让它做一个动作，做的好就给肉吃（奖励），做的不好就挨打（惩罚）。通过一段时间的学习，这只狗就知道如何获得奖励不挨打。这整个训练的过程，狗是无法与人语言沟通的，在最开始并不知道应该怎么做可以吃肉，它是完全经过自己的经历和体验获得的这种“智慧”。

再如下图，是经典的RL模型，机器人做决定，执行行为（actions），环境会对行为给出反馈。这个反馈包括了新的观测结果和奖励反馈。这个反馈反作用于机器人，让它作下一次的行为决定。这个机器人和环境间互相作用，反复循环的模型就是RL。

![Fig. 1  ](/pics/DRL1-1.png  "RL"){:height="260px" width="350px"}

 **3. Deep RL 现在可以做什么？** \\
 答： 1) 通过简单和已知的法则，在某些领域得到高水平的熟练程度 2）只用简单的输入设备，通过足够多的经验训练，学习简单的行为 3）学习人类提供的专家行为。

**4. 为什么要深度学习呢？** \\
答：**因为深度模型能帮助增强学习算法“端对端”(end-to-end)地解决复杂的问题。**

要理解”端对端“，可以看下面两张图。图1中，上面一行是一个标准的机器视觉算法例子：输入图像，提取特征，提取中层特征，分类器输出分类结果。下面一行是一个深度学习“端对端”的例子：输入图像，通过深度神经网络，直接输出分类结果。当然，这个深度神经网络是经过大量的图像-类别训练过的。
![Fig. 2  ](/pics/DRL1-2.png  "end to end1")

图2是在机器人领域的一个例子。上面一行是一个标准的机器人控制流程：观测数据，状态估计，模型预测，规划，低水平的控制，输出机器人控制信号。下面一行是深度学习“端对端”的例子：输入观测值，通过神经网络，直接输出机器人控制信号。和机器视觉那个例子一样，这个深度网络也是需要通过大量的观测值-机器人控制信号的训练样本训练。
![Fig. 3 ](/pics/DRL1-3.png  "end to end2")

**5. 什么时候要用增强学习呢？** \\
答：序列决策的时候。什么是序列决策呢？当你的系统是单一独立的决定，不影响到未来的决定的时候，就不需要序列决策，比如分类、逻辑回归。相反，当你的当前决定影响到未来决定的时候，你就需要考虑序列决策，比如机器人控制、自动车驾驶、语言对话和金融模型。

 **6. 为什么现在要学习Deep RL呢？**\\
 答： 因为深度学习，增强学习以及计算能力的最新进展，创造了特别好的环境，Deep RL 是应运而生。
 
 **7. 除了增强学习，还有其他形式的监督式学习吗?**\\
 答： 1) 模仿学习(Learning from demonstrations): 直接从观测行为中推断奖励。2) 从观测世界中学习(Learning from observing the world): 非监督式的方式学习预测。3) 任务学习（Learning from other tasks): 迁移学习，学习如何学习。
 
 **8. Deep RL 现在可以做什么？** \\
 答： 1) 通过简单和已知的法则，在某些领域得到高水平的熟练程度 2）只用简单的输入设备，通过足够多的经验训练，学习简单的行为 3）学习人类提供的专家行为。
 
  **9. Deep RL 的挑战是什么？** \\
 答： 1) 人类学的很快，而Deep RL通常很慢 2）人类可以重新使用以前的只是，而迁移学习还是一个问题 3）奖励机制和预测模型应该如何设计，还不清楚。 
 

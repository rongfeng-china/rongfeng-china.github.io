---
layout: post
title: UCL增强学习课程笔记[2]
date: 2017-11-29 20:45:00 +01:00
categories: tech
tags: RL
---
## 1: Introduction to Reinforcement Learning
课程网站[课件+视频](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html)

## 1. 机器学习的分支：
1.监督式学习（有标签），2.非监督式学习（无标签），3.增强学习（无标签，有环境反馈) \\
增强学习和其他方法不同之处在于：

* 不像监督式学习有标签，它只有奖励信号(reward signal)
* 反馈是延迟的，而不是立刻的
* 时间很重要
* agent（可以理解为机器人，或者在图2中的人脑）的行为影响接下来一系列接受到的反馈数据
![ ](/pics/L1-1.png  "machine learning branches" ){:height="260px" width="350px"}

## 2. 增强学习基本概念
典型的增强学习场景：Agent（人脑）控制动作，这些动作作用于环境（地球），环境给予反馈。人脑通过观察环境，和分析奖惩，再做下一个决定。*这在日常生活中很常见，也很容易理解。比如你这一秒在写作业，下一秒决定去玩（动作），结果会被爸妈打（反馈），所以被迫回来继续写作业（下一个动作）。当然了，你不会一下子就那么听话的，经过多次反馈，你就学会了对于自己有益的策略。*

* 在时刻t的时候, agent人脑：
	* 执行动作 $ A_t $
	* 接收到环境的观测值$O_t$
	* 接收到环境的奖励$R_t$
* 环境：
	* 接收到行为$A_t$
	* 释放观测信号$O_{t+1}$
	* 释放奖励信号$R_{t+1}$
* t 时间增加
![ ](/pics/L1-2.png  "RL basic concetps" )


## 3. 对于变量的解释
随着时间的增长，你会有很多关于观测$O_t$，行为$A_t$和奖励$R_t$的序列，我们可以称它为历史$H_t$。
$$H_t = O_1, R_1, A_1, ... , A_{t-1}, O_{t-1},R_{t-1}$$
在增强学习里，你常会遇见一个$S_t$状态变量，这个状态变量就是对于历史的综合处理$S_t = f(H_t)$， 它可以用来对未来发生的事情做判断和决定。我们不是常说从历史中吸取经验教训嘛，就是这个道理。

*其实我很佩服Rich Sutton，他提出的增强学习模型很简单，但是很贴近人脑的运作方式。比如说从历史中学习经验教训，还有explore和exploit的行为模式（人在有些时候会探索未知、尝试新鲜的事情，另外一些时候我们会采取对于自己最优的策略。）从某种意义上，我觉得他的算法是很哲学的思考的结果。*

$S_t$具体分有环境的状态$S_t^e$和Agent的状态$S_t^a$，当环境是完全可观测的（fully observable)，也就是Agent能直接看到环境的状态的时候，$S_t^e$和$S_t^a$就是等同的。然而，如果环境是部分可观测的(partially observable)，Agent获得到的就不是直接的环境状态。

## 4. 增强学习Agent的组成 
一个增强学习的Agent可能包括一个或者更多个以下的组成部分：
* **策略（Policy)**: Agent如何做行为决策
* **价值函数(Value Function)**: 判断一个状态 与/或 一个行为的好坏
* **模型(Model)**: Agent对于环境的表达

**策略（Policy)**
* 它是Agent的行为方式。
* 给定一个状态S，经过策略，可以得到一个行为A。所以策略是从状态到行为的映射。
* 决定策略 (Determitistic Policy),在状态S下，Agent一定会执行动作a。\\
$a = \pi(s)$
* 随机策略 (Stochastic Policy)， 在状态s下，Agent执行a的概率。\\
$\pi(a|s)=p[A_t = a| S_t = s] $

**价值函数(Value Function)**
* 它是对于未来奖惩的预测
* 被用于评估一个状态的好坏
* 因此被用于选择行为A，例如以下是对于未来获得奖励的一个期望：\\
$V_{\pi}(s) = E_{\pi}[ R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+.... | S_t = s]$


**模型(Model)**:
* 它是用于预测接下来环境会做什么
* P 预测下一个状态
* R 预测下一个即刻奖惩 (reward),例如：
$ P_{ss'}^a = P[S_{t+1}= s' | S_t = s , A_t = a ]$\\
$ R_s^a = E [R_{t+1}| S_t = s, A_t = a ]$ 


接下来课件里举了一个迷宫的例子(Maze example)，帮助你理解之前介绍的所有概念，可以跟着课件细细看，这里就不细讲了。

## 5. 增强学习的分类
* 基于价值(value based)
	* 价值函数
* 基于策略(policy based)
	* 策略方法
* 演员评判家 (Actor Critic )
	* 既有价值函数，又有策略方法
	
另一种是根据模型来分类：Model Free是指不需要去猜测environment的工作方式，而Model based则是需要学习environment的工作方式。
	
![ ](/pics/L1-3.png  "RL categorizing")

## 6. 增强学习和规划(planning)的异同
* 增强学习
	* 环境是部分已知
	* Agent和环境交互
	* Agent改进自己的策略
* 规划
	* 环境模型已知
	* Agent根据模型进行计算（不交互）
	* Agent 改进自己的策略
	
## 7. 探索和利用(Exploration and Exploitation)
总而言之，增强学习是一种试错学习（trial-and-error), Agent能够通过和环境交互中的经验，发现一个好的策略。增强学习选择动作，包括探索和利用。探索是为了发现更多关于环境的信息，利用是为了充分利用已有的经验而最大化奖励。

[更多RL文章](http://localhost:4000/2017/11/RL_notes/) 
